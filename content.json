{"meta":{"title":"Numberman","subtitle":null,"description":null,"author":"Kenny","url":"http://yoursite.com"},"pages":[{"title":"categories","date":"2018-07-19T08:34:06.000Z","updated":"2018-07-19T08:42:30.906Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-06-19T09:40:05.000Z","updated":"2018-07-19T08:47:07.048Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Gitea","slug":"Gitea","date":"2018-07-13T06:09:17.000Z","updated":"2018-07-13T06:59:34.627Z","comments":true,"path":"2018/07/13/Gitea/","link":"","permalink":"http://yoursite.com/2018/07/13/Gitea/","excerpt":"Gitea 介紹Gitea 是一個可自行託管的 Git 服務。","text":"Gitea 介紹Gitea 是一個可自行託管的 Git 服務。 環境要求Ubuntu 16.04 LTS Docker 1.10+ 安裝 Gitea# 下載映像檔1docker pull gitea/gitea:latest # 建立資料目錄1sudo mkdir -p /var/lib/gitea # 啟動容器1docker run -d --name=gitea -p 10022:22 -p 10080:3000 -v /var/lib/gitea:/data gitea/gitea:latest 此處10080可改成80 # 訪問介面 http://hostname:10080 初次訪問需初始化設定 伺服器和其他服務設定 管理員帳號設定 # LDAP設定 登入後，網站管理 認證來源&gt;新增認證來源 即可使用AD帳密登入","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"Helm","slug":"Helm","date":"2018-07-02T06:25:57.000Z","updated":"2018-07-03T07:56:24.772Z","comments":true,"path":"2018/07/02/Helm/","link":"","permalink":"http://yoursite.com/2018/07/02/Helm/","excerpt":"Helm 介紹Helm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。隨著容器化與微服務架構的出現，帶給我們便利的同時，應用被拆分成多個組件，導致服務數量大幅增加，對於 Kubernetes 來說，每個組件有自己的資源文件，並且可以獨立進行佈署與伸縮，而 Helmd 可以簡化這種模式在佈署與管理的複雜性。 Helm 把 Kubernetes 資源打包到一個 chart 中，而 chart 被保存到 chart 倉庫。通過 chart 倉庫可用來儲存和分享 chart 。Helm 使應用發佈可配置，支持版本管理，簡化了 Kubernetes 佈署應用的版本控制、打包、發佈、刪除、更新、退版等操作。","text":"Helm 介紹Helm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。隨著容器化與微服務架構的出現，帶給我們便利的同時，應用被拆分成多個組件，導致服務數量大幅增加，對於 Kubernetes 來說，每個組件有自己的資源文件，並且可以獨立進行佈署與伸縮，而 Helmd 可以簡化這種模式在佈署與管理的複雜性。 Helm 把 Kubernetes 資源打包到一個 chart 中，而 chart 被保存到 chart 倉庫。通過 chart 倉庫可用來儲存和分享 chart 。Helm 使應用發佈可配置，支持版本管理，簡化了 Kubernetes 佈署應用的版本控制、打包、發佈、刪除、更新、退版等操作。 Helm 架構 Helm 概念# Chart 包含 Kubernetes 應用程序資源、佈署檔案的集合，為 Helm 的打包格式 # Release 在 Kubernetes 中運行的 Chart 實例，類似 Kubernetes 的 Deployment # Repository Helm的軟體倉庫，可儲存 Chart 軟體包已供下載，並有 Chart 包的清單檔案可查詢 Helm 元件# Helm Client 一個安裝 Helm CLI 的機器，該機器透過 gRPC 連接 Tiller Server 來對 Repository、Chart 與 Release 等進行管理與操作，如建立、刪除與升級等操作 # Tiller Server 主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 佈署 Helm 安裝# 安裝 Helm Client123curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get &gt; get_helm.shchmod 700 get_helm.sh./get_helm.sh # 創建 Tiller 的 serviceaccount 和 clusterrolebinding123kubectl create serviceaccount --namespace kube-system tillerkubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller # 或是使用yaml，內容如下123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system # 將上面內容存成helmsec.yaml1vim helmsec.yaml # 創建對應的 service account 和 role binding1kubectl create -f helmsec.yaml # 安裝 Tiller Server1helm init --service-account tiller # 確認版本1helm version # 舊版升級1helm init --upgrade","categories":[],"tags":[{"name":"Helm","slug":"Helm","permalink":"http://yoursite.com/tags/Helm/"}]},{"title":"Ubuntu 網路設定","slug":"Ubuntu-基礎設定","date":"2018-06-29T08:29:35.000Z","updated":"2018-07-02T04:06:09.677Z","comments":true,"path":"2018/06/29/Ubuntu-基礎設定/","link":"","permalink":"http://yoursite.com/2018/06/29/Ubuntu-基礎設定/","excerpt":"修改Hostname# 設定 Hostname1vim /etc/hostname # 設定Hosts1vim /etc/hosts","text":"修改Hostname# 設定 Hostname1vim /etc/hostname # 設定Hosts1vim /etc/hosts # 重新啟動1./etc/init.d/hostname.sh # 確認修改成功1hostname 修改IP、DNS# 修改 Ethernet 網路設定、設定DNS123456789101112131415161718192021sudo vi /etc/network/interfaces# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto ens160iface ens160 inet static # 固定 (靜態) IP address 10.140.20.202 # IP 位址 netmask 255.255.255.0 # 網路遮罩 gateway 10.140.20.1 # 預設閘道dns-nameservers 168.95.1.1 # 主要的 DNS 伺服器位址dns-nameservers 8.8.8.8 # 次要的 DNS 伺服器位址 # dns-* options are implemented by the resolvconf package, if installe # 重新啟動網路1sudo /etc/init.d/networking restart","categories":[],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"}]},{"title":"Harbor","slug":"Harbor","date":"2018-06-20T07:01:51.000Z","updated":"2018-07-11T06:57:47.312Z","comments":true,"path":"2018/06/20/Harbor/","link":"","permalink":"http://yoursite.com/2018/06/20/Harbor/","excerpt":"環境要求Ubuntu 16.04 LTS Python 2.7+ Docker 1.10+ Docker-compose 1.6.0+","text":"環境要求Ubuntu 16.04 LTS Python 2.7+ Docker 1.10+ Docker-compose 1.6.0+ 環境前置安裝# Check Version123ubuntu@ubuntu-xenial:~$ uname -aubuntu@ubuntu-xenial:~$ cat /etc/lsb-release # Install Python2.x123ubuntu@ubuntu-xenial:~$ sudo apt-get updateubuntu@ubuntu-xenial:~$ sudo apt-get install -y python # Check Python Version1ubuntu@ubuntu-xenial:~$ python --version # Install Docker1ubuntu@ubuntu-xenial:~$ sudo apt-get install -y docker.io # Check Docker Version1ubuntu@ubuntu-xenial:~$ sudo docker version # Install Docker Compose1ubuntu@ubuntu-xenial:~$ sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.11.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose # 設定權限1ubuntu@ubuntu-xenial:~$ sudo chmod +x /usr/local/bin/docker-compose # Check Docker Compose Version1ubuntu@ubuntu-xenial:~$ docker-compose --version # /etc/ssl/openssl.cnf內的[v3_ca]加入倉庫IP123ubuntu@ubuntu-xenial:~$ sudo vim /etc/ssl/openssl.cnf [ v3_ca ] subjectAltName=IP:xx.xx.xx.xx 安裝 Harbor# 下載版本v1.1.11ubuntu@ubuntu-xenial:~$ wget https://github.com/vmware/harbor/releases/download/v1.1.1/harbor-online-installer-v1.1.1.tgz # 解壓縮1ubuntu@ubuntu-xenial:~$ tar zxvf harbor-online-installer-v1.1.1.tgz # Create Directory for Certificate and Change Directory123ubuntu@ubuntu-xenial:~$ mkdir certubuntu@ubuntu-xenial:~$ cd cert # Create Certificate # Input Common Name only at this time1ubuntu@ubuntu-xenial:~/cert$ openssl req -sha256 -x509 -days 365 -nodes -newkey rsa:4096 -keyout registry.08online.xsg.key -out registry.08online.xsg.crt 12345678910111213Country Name (2 letter code) [AU]:State or Province Name (full name) [Some-State]:Locality Name (eg, city) []:Organization Name (eg, company) [Internet Widgits Pty Ltd]:Organizational Unit Name (eg, section) []:Common Name (e.g. server FQDN or YOUR name) []:IPEmail Address []: # Change Directory and Modify harbor.cfg123ubuntu@ubuntu-xenial:~$ cd harborubuntu@ubuntu-xenial:~/harbor$ vim harbor.cfg 123456789101112131415161718192021&lt; hostname = reg.mydomain.com&gt; hostname = registry.08online.xsg &lt; ui_url_protocol = http&gt; ui_url_protocol = https &lt; ssl_cert = /data/cert/server.crt&gt; ssl_cert = /home/ubuntu/cert/registry.08online.xsg.crt &lt; ssl_cert_key = /data/cert/server.key&gt; ssl_cert_key = /home/ubuntu/cert/registry.08online.xsg.key # Harbor has been installed1ubuntu@ubuntu-xenial:~/harbor$ sudo ./install.sh # Check Containers for Harbor1ubuntu@ubuntu-xenial:~/harbor$ sudo docker-compose top #WebUI(https://IP) 帳號：admin 密碼：xxxxx 安裝 Certificate# 修改憑證，需用公司憑證取代 123ubuntu@ubuntu-xenial:~/harbor$ vim /home/ubuntu/cert/registry.08online.xsg.crt ubuntu@ubuntu-xenial:~/harbor$ vim /home/ubuntu/cert/registry.08online.xsg.key # 修改Docker login需要之憑證 1234567ubuntu@ubuntu-xenial:~/harbor$ mkdir -p /etc/docker/certs.d/registry.08online.xsg/ ubuntu@ubuntu-xenial:~/harbor$ vim /etc/docker/certs.d/registry.08online.xsg/ca.crt ubuntu@ubuntu-xenial:~/harbor$ vim /etc/docker/certs.d/registry.08online.xsg/client.certubuntu@ubuntu-xenial:~/harbor$ vim /etc/docker/certs.d/registry.08online.xsg/client.key # 測試登入1ubuntu@ubuntu-xenial:~/harbor$ docker login registry.08online.xsg 帳號：admin 密碼：xxxxxx LDAP設定# 修改設定檔 1234567891011121314151617181920212223242526272829303132333435363738ubuntu@ubuntu-xenial:~/harbor$ vim harbor.cfg &lt; auth_mode = db_auth&gt; auth_mode = ldap_auth &lt; ldap_url = ldaps://ldap.mydomain.com&gt; ldap_url = ldap://xxx.xxx.xxx:xxx &lt; #ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com&gt; ldap_searchdn = CN=xxxxxxxx,OU=PublicID,OU=Account,DC=xxx,DC=xxx &lt; #ldap_search_pwd = password&gt; ldap_search_pwd = xxxxxxxx &lt; ldap_basedn = ou=people,dc=mydomain,dc=com&gt; ldap_basedn = dc=xxx,dc=xxx &lt; ldap_uid = uid&gt; ldap_uid = sAMAccountName # 重啟服務並強制清除data目錄下資料 1234567ubuntu@ubuntu-xenial:~/harbor$ docker-compose down -vubuntu@ubuntu-xenial:~/harbor$ rm -rf /dataubuntu@ubuntu-xenial:~/harbor$ ./prepareubuntu@ubuntu-xenial:~/harbor$ docker-compose up -d # 先使用管理者帳號登入 1ubuntu@ubuntu-xenial:~/harbor$ docker login registry.08online.xsg 帳號：admin 密碼：xxxxxx # 登入Web-UI調整設定 Configuration &gt; Authentication 測試連線成功後，即可使用AD帳號登入 Push、Pull# 登入 1ubuntu@ubuntu-xenial:~/harbor$ docker login registry.08online.xsg # 幫需上傳的image加上tag 1ubuntu@ubuntu-xenial:~/harbor$ docker tag redis:latest registry.08online.xsg/redis:latest # 上傳 1ubuntu@ubuntu-xenial:~/harbor$ docker push registry.08online.xsg/library/redis:latest # 下載 1ubuntu@ubuntu-xenial:~/harbor$ docker pull registry.08online.xsg/library/redis:latest","categories":[],"tags":[{"name":"Harbor","slug":"Harbor","permalink":"http://yoursite.com/tags/Harbor/"}]},{"title":"Prometheus","slug":"Prometheus","date":"2018-06-19T10:26:03.000Z","updated":"2018-06-20T05:55:29.268Z","comments":true,"path":"2018/06/19/Prometheus/","link":"","permalink":"http://yoursite.com/2018/06/19/Prometheus/","excerpt":"Prometheus特點多維數據架構 (由 metric 名稱與 key/ value 定義的時間序列) 靈活的查詢語言(PromQL) 支援Local與Remote，不依賴分散式儲存 使用Pull方式取得目標資訊，通過HTTP協議傳輸 支援Pushgateway 支援多種圖形模式與Dashboard","text":"Prometheus特點多維數據架構 (由 metric 名稱與 key/ value 定義的時間序列) 靈活的查詢語言(PromQL) 支援Local與Remote，不依賴分散式儲存 使用Pull方式取得目標資訊，通過HTTP協議傳輸 支援Pushgateway 支援多種圖形模式與Dashboard Prometheus核心元件Prometheus Server：主要服務，用來抓取和儲存時序資料 (Time Series Data) Client librarys：用於對接Prometheus Server，負責檢測應用程序代碼，讓其可以透過HTTP傳送資訊到Prometheus Server PushGateway：支援Client主動將短期和批量的監控數據推送至此，而Prometheus會定時來抓取數據 Exporter：負責從目標收集數據，並轉換成Prometheus支援的格式，不同的Exporter負責不同的業務， 命名格式為xxx_exporter Alertmanager：用於告警通知管理 Prometheus架構 Prometheus VS InfluxDBInfluxDB：僅僅是一個資料庫，它被動的接受客戶端資料和查詢請求，基於 Push Prometheus：完整的監控系統，能抓取資料、查詢資料、告警等功能，基於 Pull Push 和 Pull 主要區別在發起者不同及邏輯架構不同 PromQL在Node Exporter的/metrics接口中返回的監控數據，在Prometheus下稱為一個樣本。採集到的樣本由以下三部分組成： 指標（metric） 時間戳記（timestamp） 樣本值（value） Metric TypeCounter（計數器） rate(http_requests_total[5m]) Gauge（儀表板） node_memory_MemFree predict_linear(node_filesystem_free{job=”kubernetes-node-exporter”}[1h], 4 * 3600) Histogram（直方圖） Summary（摘要） 環境準備Ubuntu 16.04 LTS Docker 1.10+ Kubernetes v1.9.6 Node_exporter v0.15+ Prometheus v2.0.0 Grafana 5.1+ 安裝PrometheusCreate A Namespace： 1kubectl create namespace kube-ops Create Exporter： 1kubectl create -f node-exporter.yaml node-exporter.yaml 123456789101112131415161718192021222324252627282930313233343536373839---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: node-exporter namespace: kube-ops labels: k8s-app: node-exporterspec: template: metadata: labels: k8s-app: node-exporter spec: containers: - image: prom/node-exporter name: node-exporter ports: - containerPort: 9100 protocol: TCP name: http---apiVersion: v1kind: Servicemetadata: labels: k8s-app: node-exporter name: node-exporter namespace: kube-opsspec: ports: - name: http port: 9100 nodePort: 31672 protocol: TCP type: NodePort selector: k8s-app: node-exporter Create ServiceAccount、ClusterRole、ClusterRoleBinding： 1kubectl create -f prometheus-sa.yaml prometheus-sa.yaml 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: kube-ops---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: prometheus namespace: kube-opsrules:- apiGroups: [\"\"] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"]- nonResourceURLs: [\"/metrics\"] verbs: [\"get\"]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: prometheus namespace: kube-opsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: kube-ops Create A Config Map： 1kubectl create -f prometheus-cm.yaml prometheus-cm.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-opsdata: prometheus.yml: | global: scrape_interval: 30s scrape_timeout: 30s rule_files: - /etc/prometheus/rules.yml alerting: alertmanagers: - static_configs: - targets: [\"localhost:9093\"] scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: 'kubernetes-cadvisor' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: 'kubernetes-node-exporter' scheme: http tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - source_labels: [__meta_kubernetes_role] action: replace target_label: kubernetes_role - source_labels: [__address__] regex: '(.*):10250' replacement: '$&#123;1&#125;:31672' target_label: __address__ - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name rules.yml: | groups: - name: test-rule rules: - alert: NodeFilesystemUsage expr: (node_filesystem_size&#123;device=\"rootfs\"&#125; - node_filesystem_free&#123;device=\"rootfs\"&#125;) / node_filesystem_size&#123;device=\"rootfs\"&#125; * 100 &gt; 80 for: 2m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: High Filesystem usage detected\" description: \"&#123;&#123;$labels.instance&#125;&#125;: Filesystem usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;\" - alert: NodeMemoryUsage expr: (node_memory_MemTotal - (node_memory_MemFree+node_memory_Buffers+node_memory_Cached )) / node_memory_MemTotal * 100 &gt; 80 for: 2m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: High Memory usage detected\" description: \"&#123;&#123;$labels.instance&#125;&#125;: Memory usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;\" - alert: NodeCPUUsage expr: (100 - (avg by (instance) (irate(node_cpu&#123;job=\"kubernetes-node-exporter\",mode=\"idle\"&#125;[5m])) * 100)) &gt; 80 for: 2m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: High CPU usage detected\" description: \"&#123;&#123;$labels.instance&#125;&#125;: CPU usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;\" - alert: test expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total&#123;job=\"kubernetes-node-exporter\",mode=\"idle\"&#125;[5m])) * 100)) &gt; 1 for: 2m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: High CPU usage detected\" description: \"&#123;&#123;$labels.instance&#125;&#125;: CPU usage is above 1% (current value is: &#123;&#123; $value &#125;&#125;\"---kind: ConfigMapapiVersion: v1metadata: name: alertmanager namespace: kube-opsdata: config.yml: |- global: resolve_timeout: 5m route: receiver: webhook group_wait: 30s group_interval: 5m repeat_interval: 4h group_by: [alertname] routes: - receiver: webhook group_wait: 10s match: team: node receivers: - name: webhook webhook_configs: - url: 'http://apollo/hooks/dingtalk/' send_resolved: true - url: 'http://apollo/hooks/prome/' send_resolved: true Create A Prometheus Deployment： 1kubectl create -f prometheus-deploy.yaml prometheus-deploy.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: k8s-app: prometheus name: prometheus namespace: kube-opsspec: replicas: 1 template: metadata: labels: k8s-app: prometheus spec: serviceAccountName: prometheus containers: - image: prom/prometheus:v2.0.0-rc.3 name: prometheus command: - \"/bin/prometheus\" args: - \"--config.file=/etc/prometheus/prometheus.yml\" - \"--storage.tsdb.path=/prometheus\" - \"--storage.tsdb.retention=24h\" ports: - containerPort: 9090 protocol: TCP name: http volumeMounts: - mountPath: \"/prometheus\" name: data - mountPath: \"/etc/prometheus\" name: config-volume resources: requests: cpu: 100m memory: 100Mi limits: cpu: 200m memory: 1Gi - image: quay.io/prometheus/alertmanager:v0.12.0 name: alertmanager args: - \"-config.file=/etc/alertmanager/config.yml\" - \"-storage.path=/alertmanager\" ports: - containerPort: 9093 protocol: TCP name: http volumeMounts: - name: alertmanager-config-volume mountPath: /etc/alertmanager resources: requests: cpu: 50m memory: 50Mi limits: cpu: 200m memory: 200Mi volumes: - name: data emptyDir: &#123;&#125; - configMap: name: prometheus-config name: config-volume - name: alertmanager-config-volume configMap: name: alertmanager Exposing Prometheus As A Service： 1kubectl create -f prometheus-svc.yaml prometheus-svc.yaml 123456789101112131415apiVersion: v1kind: Servicemetadata: name: prometheus namespace: kube-ops labels: k8s-app: prometheusspec: selector: k8s-app: prometheus type: NodePort ports: - name: web port: 9090 targetPort: http Create A Grafana Deployment： 1kubectl create -f grafana-deployment.yaml grafana-deployment.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: grafana namespace: kube-ops labels: app: grafana #component: corespec: replicas: 1 template: metadata: labels: app: grafana #component: core spec: containers: - image: grafana/grafana:latest securityContext: runAsUser: 0 name: grafana imagePullPolicy: IfNotPresent # env: resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi env: # The following env variables set up basic auth twith the default admin user and admin password. - name: GF_AUTH_BASIC_ENABLED value: \"true\" - name: GF_AUTH_ANONYMOUS_ENABLED value: \"false\" # - name: GF_AUTH_ANONYMOUS_ORG_ROLE # value: Admin # does not really work, because of template variables in exported dashboards: # - name: GF_DASHBOARDS_JSON_ENABLED # value: \"true\" readinessProbe: httpGet: path: /login port: 3000 # initialDelaySeconds: 30 # timeoutSeconds: 1 volumeMounts: - name: grafana-persistent-storage mountPath: /var/lib/grafana volumes: - name: grafana-persistent-storage emptyDir: &#123;&#125; Exposing the Grafana web UI： 1kubectl create -f grafana-svc.yaml grafana-svc.yaml 123456789101112131415apiVersion: v1kind: Servicemetadata: name: grafana namespace: kube-ops labels: app: grafana #component: corespec: type: NodePort ports: - port: 3000 selector: app: grafana #component: core Prometheus UI介面Grafana設定","categories":[],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"http://yoursite.com/tags/Prometheus/"}]},{"title":"Prometheus (Docker)","slug":"Prometheus-Docker","date":"2018-03-26T07:25:46.000Z","updated":"2018-07-19T08:39:21.384Z","comments":true,"path":"2018/03/26/Prometheus-Docker/","link":"","permalink":"http://yoursite.com/2018/03/26/Prometheus-Docker/","excerpt":"環境準備Ubuntu 16.04 LTS Docker 1.12+","text":"環境準備Ubuntu 16.04 LTS Docker 1.12+ 前置安裝# Check Version1sudo apt-get install -y docker.io # Check Docker Version1sudo docker version 安裝 mysqld_exporter# 啟動mysqld_exporter容器123docker run --name mysqld_exporter -d -p 9104:9104 --restart=always \\ -e DATA_SOURCE_NAME=&quot;exporter:exporter@(localhost:3306)/mysql&quot; \\ prom/mysqld-exporter # 查看容器日誌，確定容器正常運行1docker logs -f mysqld_exporter # 確認exporter有導出數據 瀏覽器訪問http://localhost:9104 安裝prometheus# 創建資料夾1mkdir -p /home/docker/prometheus # 創建Data資料夾1mkdir -p /home/docker/prometheus/prometheus-data # 編輯yml文件1vim /home/docker/prometheus/prometheus.yml # 配置文檔如下1234567891011121314151617181920212223242526272829303132333435# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] labels: instance: prometheus - job_name: 'mysql' scrape_interval: 15s static_configs: - targets: - 'localhost:9104' labels: instance: db1 # 啟動prometheus容器1234docker run --name prometheus -d -p 9090:9090 --restart=always \\ -v /home/docker/prometheus/prometheus-data:/prometheus-data \\ -v /home/docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus # 確認Targets 瀏覽器訪問http://localhost:9090/targets 安裝Grafana# 創建資料夾1mkdir -p /home/docker/grafana # 創建Data資料夾1mkdir -p /home/docker/grafana/data # 啟動grafana容器1234docker run --name grafana -d -p 3000:3000 --restart=always \\ -e &quot;GF_SECURITY_ADMIN_PASSWORD=admin&quot; \\ -v /home/docker/grafana/data:/var/lib/grafana \\ grafana/grafana # 登入grafana介面 瀏覽器訪問http://localhost:3000 帳號admin 密碼admin # Add deta source # 推薦Dashboards 3622 2 358 159","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://yoursite.com/categories/DevOps/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"http://yoursite.com/tags/Prometheus/"}]}]}