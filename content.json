{"meta":{"title":"Number man","subtitle":null,"description":null,"author":"Kenny","url":"http://yoursite.com"},"pages":[{"title":"Categories","date":"2018-07-19T08:34:06.000Z","updated":"2018-07-19T09:09:11.682Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-06-19T09:40:05.000Z","updated":"2018-07-19T09:59:49.227Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"ELK Stack","slug":"ELK Stack","date":"2019-01-09T02:53:20.000Z","updated":"2020-02-24T05:34:50.979Z","comments":true,"path":"2019/01/09/ELK Stack/","link":"","permalink":"http://yoursite.com/2019/01/09/ELK Stack/","excerpt":"ELK 介紹ELK是由 ElasticSearch、Logstash、Kibana 三個 open-source 軟體組成。運作流程為，Logstash 從 Server 端收集了 Log 並將之處理後，將其推送至 ElasticSearch 進行儲存，再透過 Kibana 視覺化界面呈現出來，可在其介面上搜尋並分析 Log。","text":"ELK 介紹ELK是由 ElasticSearch、Logstash、Kibana 三個 open-source 軟體組成。運作流程為，Logstash 從 Server 端收集了 Log 並將之處理後，將其推送至 ElasticSearch 進行儲存，再透過 Kibana 視覺化界面呈現出來，可在其介面上搜尋並分析 Log。 # ElasticSearch 分布式搜尋引擎，可用於日誌的檢索 # Logstash 數據收集處理引擎，可用於收集日誌或是資訊，並根據設定的格式，轉換成該資料欄位 # Kibana 是一款可為 ElasticSearch 提供圖表分析與搜尋的 Web 介面 Beats 介紹Beats 是 open-source 的數據發送者，可安裝在 Server 端上，將各種類型的數據發送到 ElasticSearch ，Beats 可以直接發送數據到 ElasticSearch 或通過 Logstash 發送到 ElasticSearch ，為何這種使用方式是因為 Logstash 若安裝在 Server 端上，會使其 CPU 及記憶體消耗過多造成效能下降，而 Beats 所佔系統的效能幾乎可以忽略不計。目前 Beats 包含四種： # Packetbeat 搜集網路流量數據 # Topbeat 搜集系統、進程和文件系統級別的 CPU 和記憶體使用情況等數據 # Filebeat 搜集文件數據 # Winlogbeat 搜集 Windows 事件日誌數據 ELK 架構若系統架構不大，可使用 Filebeat + Logstash 並行的架構 此架構讓 Filebeat 單純用來接收 Log ，讓其他較複雜的交給 Logstash 來處理\\\\\\若系統架構龐大， Log 不能有所遺失，那上述的架構便不符需求了， Logstash 的效能並不好，忙碌時容易造成 Log 遺失，即使 Filebeat 有重送的功能，若 Server 端剛好執行登錄檔輪替( logrotate ) ， Log 還沒被送到 Logstash 就被壓縮起來了。為避免此狀況，我們可以在中介加入 Redis 。 此架構先把 Log 都先送到 Redis ，等 Logstash 有空就可以從 Redis 將資料抓出來歸檔，當然中介不一定要使用 Redis ， kafka 或是 RabbitMQ 也行，這邊選擇 Redis 是因為 Redis 的數據全部 in memory ，所以速度較快。 ELK 安裝建置# 機器規格 項目 規格 CPU 12 Core RAM 24G System Disk 50G HDD Data Disk 100G HDD OS Ubuntu 16.04 LTS # 更新並安裝 Docker12sudo apt-get updatesudo apt-get install -y docker.io # 建立 Redis1docker run -d --name elk-redis -p 6379:6379 -v /data/redis:/data --restart=always redis:3.2.4 redis-server --maxmemory 4gb --requirepass QFkXXBZkLD6MgcEL1y8l 指令 說明 -d 在背景執行 –name Container 的名稱 -p 本機與 Container 對應的 port -v 本機與 Container 對應的資料路徑 –restart=always 機器重啟後 Container 自動重啟（預設是關閉） redis:3.2.4 Redis Container 的映像檔名稱與版本 redis-server 建立 redis 的 server（另有 redis-cli 的客端版本） –maxmemory 4gb 設定 Redis Container 佔用記憶體的最大容量 –requirepass 客端 VM 將 Log 資料傳送到 Redis 時驗證用的 key \\# 建立 Elasticsearch1docker run -d --name elk-elasticsearch -p 9200:9200 -v /data/elasticsearch:/usr/share/elasticsearch/data --restart=always -e ES_JAVA_OPTS:-Xmx6g -e ES_JAVA_OPTS:-Xms6g elasticsearch:5.3.1 指令 說明 -d 在背景執行 –name Container 的名稱 -p 本機與 Container 對應的 port -v 本機與 Container 對應的資料路徑 –restart=always 機器重啟後 Container 自動重啟（預設是關閉） -e ES_JAVA_OPTS:-Xms 指定Elasticsearch 占用記憶體的初始值 -e ES_JAVA_OPTS:-Xmx 指定Elasticsearch 占用記憶體的最大值 elasticsearch:5.3.1 Elasticsearch 的映像檔名稱與版本 \\# 建立 Logstash 建立 Logstash 資料夾1mkdir logstash 進入 logstash 資料夾，建立 conf.d 資料夾12cd logstash mkdir conf.d 建立 Dockerfile12345vim DockerfileFROM logstash:5.3.1COPY conf.d /etc/logstash/conf.dCMD [&quot;-f&quot;, &quot;/etc/logstash/conf.d&quot;] 進入 conf.d 資料夾，建立 logstash.conf1vim logstash.conf 123456789101112131415input &#123; redis &#123; host =&gt; &quot;redis&quot; port =&gt; 6379 data_type =&gt; &quot;list&quot; key =&gt; &quot;08log&quot; password =&gt; &quot;QFkXXBZkLD6MgcEL1y8l&quot; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;elasticsearch&quot;] index =&gt; &quot;%&#123;[@metadata][env]&#125;_%&#123;[type]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;&#125; 建立 weblog.conf1vim weblog.conf 12345678910111213141516171819202122232425filter &#123; if [type] == &quot;weblog&quot; &#123; grok &#123; match =&gt; [&quot;message&quot;, &quot;%&#123;TIMESTAMP_ISO8601:[@metadata][timestamp]&#125; %&#123;NUMBER:threadid&#125; %&#123;LOGLEVEL:loglevel&#125; %&#123;NOTSPACE:logger&#125; %&#123;GREEDYDATA:message&#125;&quot;] overwrite =&gt; [ &quot;message&quot; ] &#125; date &#123; match =&gt; [ &quot;[@metadata][timestamp]&quot;, &quot;YYYY-MM-dd HH:mm:ss.SSS&quot; ] timezone =&gt; &quot;UTC&quot; &#125; mutate &#123; convert =&gt; &#123; &quot;threadid&quot; =&gt; &quot;integer&quot; &#125; add_field =&gt; &#123; &quot;hostname&quot; =&gt; &quot;%&#123;[beat][hostname]&#125;&quot; &quot;webtype&quot; =&gt; &quot;%&#123;[fields][webtype]]&#125;&quot; &quot;[@metadata][env]&quot; =&gt; &quot;%&#123;[fields][env]]&#125;&quot; &#125; remove_field =&gt; [&quot;beat&quot;, &quot;fields&quot;] &#125; &#125;&#125; 回到 logstash 資料夾，建立鏡像1docker build -t elk-logstash . 開始安裝 Logstash1docker run -d --name elk-logstash -p 5000:5000 --link elk-redis:redis --link elk-elasticsearch:elasticsearch --restart=always elk-logstash:latest 指令 說明 -d 在背景執行 –name Container 的名稱 -p 本機與 Container 對應的 port –link 其它的 Container 做連結 –restart=always 機器重啟後 Container 自動重啟（預設是關閉） -e ES_JAVA_OPTS:-Xms 指定Elasticsearch 占用記憶體的初始值 -e ES_JAVA_OPTS:-Xmx 指定Elasticsearch 占用記憶體的最大值 elk-logstash:latest 自建 logstash 映像檔名稱與版本 \\# 建立 Kibana1docker run -d --name elk-kibana -p 80:5601 --link elk-elasticsearch:elasticsearch --restart=always kibana:5.3.1 指令 說明 -d 在背景執行 –name Container 的名稱 -p 本機與 Container 對應的 port –link 其它的 Container 做連結 –restart=always 機器重啟後 Container 自動重啟（預設是關閉） -e ES_JAVA_OPTS:-Xms 指定Elasticsearch 占用記憶體的初始值 -e ES_JAVA_OPTS:-Xmx 指定Elasticsearch 占用記憶體的最大值 kibana:5.3.1 kibana 的映像檔名稱與版本 \\# 建立 Grafana1docker run -d -p 3000:3000 --restart=always -v /data/grafana:/var/lib/docker/grafana --name=grafana grafana/grafana 檢查以上 Container 是否建立成功1docker ps -a 瀏覽器輸入機器 IP 即可進入初始頁面 # 建立 Filebeat 至此下載 Filebeat 5.3.1\\https://www.elastic.co/downloads/past-releases/filebeat-5-3-1 設定 filebeat.yaml 項目 說明 env Kibana Fields 的名稱 項目 說明 hosts ELK 機器 IP key logstash.conf 內 input 底下 key 值 password logstash.conf 內 input 底下的 password 值 其餘 Web 、 Server 的 Log 格式請依需求設定 將 filebeat-5.3.1-linux-x86_64 放在 D:\\Service\\ 以系統管理員開啟 Windows Power Shell，輸入以下指令執行安裝 filebeat 服務123cd D:cd Service\\filebeat-5.3.1-windows-x86_64.\\install-service-filebeat 檢查 filebeat Service 是否成功執行 # Kibana 介面設定 至 Dev Tools 輸入1GET _cat/indices?v 可看到 index 的值為 weblog ，將前面 weblog- 的複製，貼上到 Management → Index Patterns → Index 點擊 Discover 就可以看到匯入的 web Log 資料 # 刪除功能 若因匯入格式不正確的 Log 而出現 Elasticsearch 在初始化 index 導致 Kibana 無法正常使用時，請輸入下列指令刪除目前 Kibana 內目前的 Index1curl -XDELETE http://localhost:9200/*","categories":[],"tags":[]},{"title":"Portainer-Remote","slug":"Portainer-Remote","date":"2018-07-24T03:16:55.000Z","updated":"2018-07-24T03:50:17.314Z","comments":true,"path":"2018/07/24/Portainer-Remote/","link":"","permalink":"http://yoursite.com/2018/07/24/Portainer-Remote/","excerpt":"Portainer 安裝# 安裝Docker1sudo apt-get install -y docker.io","text":"Portainer 安裝# 安裝Docker1sudo apt-get install -y docker.io # 下載Image1docker pull portainer/portainer # 創建volume1docker volume create portainer_data # 安裝Portainer1docker run -d -p 9000:9000 --restart=always --name portainer -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer 此處 9000:9000 可自行修改，第一個為Host Port，第二個為Container Port # UI設定1http://localhost:9000 # 設定密碼 # 選擇Remote 輸入機器名稱與機器IP，此處 Port:2375 需打開 開啟方式 CentOS 7：12345678910vim /usr/lib/systemd/system/docker.service[Service]ExecStart=ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.socksystemctl daemon-reloadsystemctl restart docker Ubuntu 16.04：12345vim /etc/default/dockerDOCKER_OPTS=&quot;-H tcp://0.0.0.0:2375&quot;service docker restart 查詢是否成功開啟可用此指令1netstat -tulp","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://yoursite.com/categories/DevOps/"}],"tags":[{"name":"Portainer","slug":"Portainer","permalink":"http://yoursite.com/tags/Portainer/"}]},{"title":"Gitea","slug":"Gitea","date":"2018-07-13T06:09:17.000Z","updated":"2018-11-07T01:51:02.733Z","comments":true,"path":"2018/07/13/Gitea/","link":"","permalink":"http://yoursite.com/2018/07/13/Gitea/","excerpt":"Gitea 介紹Gitea 是一個可自行託管的 Git 。","text":"Gitea 介紹Gitea 是一個可自行託管的 Git 。 環境要求Ubuntu 16.04 LTS Docker 1.10+ 安裝 Gitea# 下載映像檔1docker pull gitea/gitea:latest # 建立資料目錄1sudo mkdir -p /var/lib/gitea # 啟動容器1docker run -d --name=gitea -p 10022:22 -p 10080:3000 -v /var/lib/gitea:/data gitea/gitea:latest 此處10080可改成80 # 訪問介面 http://hostname:10080 初次訪問需初始化設定 伺服器和其他服務設定 管理員帳號設定 # LDAP設定 登入後，網站管理 認證來源&gt;新增認證來源 即可使用AD帳密登入","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"Helm","slug":"Helm","date":"2018-07-02T06:25:57.000Z","updated":"2018-07-24T03:18:10.478Z","comments":true,"path":"2018/07/02/Helm/","link":"","permalink":"http://yoursite.com/2018/07/02/Helm/","excerpt":"Helm 介紹Helm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。隨著容器化與微服務架構的出現，帶給我們便利的同時，應用被拆分成多個組件，導致服務數量大幅增加，對於 Kubernetes 來說，每個組件有自己的資源文件，並且可以獨立進行佈署與伸縮，而 Helmd 可以簡化這種模式在佈署與管理的複雜性。 Helm 把 Kubernetes 資源打包到一個 chart 中，而 chart 被保存到 chart 倉庫。通過 chart 倉庫可用來儲存和分享 chart 。Helm 使應用發佈可配置，支持版本管理，簡化了 Kubernetes 佈署應用的版本控制、打包、發佈、刪除、更新、退版等操作。","text":"Helm 介紹Helm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。隨著容器化與微服務架構的出現，帶給我們便利的同時，應用被拆分成多個組件，導致服務數量大幅增加，對於 Kubernetes 來說，每個組件有自己的資源文件，並且可以獨立進行佈署與伸縮，而 Helmd 可以簡化這種模式在佈署與管理的複雜性。 Helm 把 Kubernetes 資源打包到一個 chart 中，而 chart 被保存到 chart 倉庫。通過 chart 倉庫可用來儲存和分享 chart 。Helm 使應用發佈可配置，支持版本管理，簡化了 Kubernetes 佈署應用的版本控制、打包、發佈、刪除、更新、退版等操作。 Helm 架構 Helm 概念# Chart 包含 Kubernetes 應用程序資源、佈署檔案的集合，為 Helm 的打包格式 # Release 在 Kubernetes 中運行的 Chart 實例，類似 Kubernetes 的 Deployment # Repository Helm的軟體倉庫，可儲存 Chart 軟體包已供下載，並有 Chart 包的清單檔案可查詢 Helm 元件# Helm Client 一個安裝 Helm CLI 的機器，該機器透過 gRPC 連接 Tiller Server 來對 Repository、Chart 與 Release 等進行管理與操作，如建立、刪除與升級等操作 # Tiller Server 主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 佈署 Helm 安裝# 安裝 Helm Client123curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get &gt; get_helm.shchmod 700 get_helm.sh./get_helm.sh # 創建 Tiller 的 serviceaccount 和 clusterrolebinding123kubectl create serviceaccount --namespace kube-system tillerkubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller # 或是使用yaml，內容如下123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system # 將上面內容存成helmsec.yaml1vim helmsec.yaml # 創建對應的 service account 和 role binding1kubectl create -f helmsec.yaml # 安裝 Tiller Server1helm init --service-account tiller # 確認版本1helm version # 舊版升級1helm init --upgrade","categories":[],"tags":[{"name":"Helm","slug":"Helm","permalink":"http://yoursite.com/tags/Helm/"}]},{"title":"Ubuntu 網路設定","slug":"Ubuntu-基礎設定","date":"2018-06-29T08:29:35.000Z","updated":"2018-07-02T04:06:09.677Z","comments":true,"path":"2018/06/29/Ubuntu-基礎設定/","link":"","permalink":"http://yoursite.com/2018/06/29/Ubuntu-基礎設定/","excerpt":"修改Hostname# 設定 Hostname1vim /etc/hostname # 設定Hosts1vim /etc/hosts","text":"修改Hostname# 設定 Hostname1vim /etc/hostname # 設定Hosts1vim /etc/hosts # 重新啟動1./etc/init.d/hostname.sh # 確認修改成功1hostname 修改IP、DNS# 修改 Ethernet 網路設定、設定DNS123456789101112131415161718192021sudo vi /etc/network/interfaces# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto ens160iface ens160 inet static # 固定 (靜態) IP address 10.140.20.202 # IP 位址 netmask 255.255.255.0 # 網路遮罩 gateway 10.140.20.1 # 預設閘道dns-nameservers 168.95.1.1 # 主要的 DNS 伺服器位址dns-nameservers 8.8.8.8 # 次要的 DNS 伺服器位址 # dns-* options are implemented by the resolvconf package, if installe # 重新啟動網路1sudo /etc/init.d/networking restart","categories":[],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"}]},{"title":"Harbor","slug":"Harbor","date":"2018-06-20T07:01:51.000Z","updated":"2018-07-11T06:57:47.312Z","comments":true,"path":"2018/06/20/Harbor/","link":"","permalink":"http://yoursite.com/2018/06/20/Harbor/","excerpt":"環境要求Ubuntu 16.04 LTS Python 2.7+ Docker 1.10+ Docker-compose 1.6.0+","text":"環境要求Ubuntu 16.04 LTS Python 2.7+ Docker 1.10+ Docker-compose 1.6.0+ 環境前置安裝# Check Version123ubuntu@ubuntu-xenial:~$ uname -aubuntu@ubuntu-xenial:~$ cat /etc/lsb-release # Install Python2.x123ubuntu@ubuntu-xenial:~$ sudo apt-get updateubuntu@ubuntu-xenial:~$ sudo apt-get install -y python # Check Python Version1ubuntu@ubuntu-xenial:~$ python --version # Install Docker1ubuntu@ubuntu-xenial:~$ sudo apt-get install -y docker.io # Check Docker Version1ubuntu@ubuntu-xenial:~$ sudo docker version # Install Docker Compose1ubuntu@ubuntu-xenial:~$ sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.11.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose # 設定權限1ubuntu@ubuntu-xenial:~$ sudo chmod +x /usr/local/bin/docker-compose # Check Docker Compose Version1ubuntu@ubuntu-xenial:~$ docker-compose --version # /etc/ssl/openssl.cnf內的[v3_ca]加入倉庫IP123ubuntu@ubuntu-xenial:~$ sudo vim /etc/ssl/openssl.cnf [ v3_ca ] subjectAltName=IP:xx.xx.xx.xx 安裝 Harbor# 下載版本v1.1.11ubuntu@ubuntu-xenial:~$ wget https://github.com/vmware/harbor/releases/download/v1.1.1/harbor-online-installer-v1.1.1.tgz # 解壓縮1ubuntu@ubuntu-xenial:~$ tar zxvf harbor-online-installer-v1.1.1.tgz # Create Directory for Certificate and Change Directory123ubuntu@ubuntu-xenial:~$ mkdir certubuntu@ubuntu-xenial:~$ cd cert # Create Certificate # Input Common Name only at this time1ubuntu@ubuntu-xenial:~/cert$ openssl req -sha256 -x509 -days 365 -nodes -newkey rsa:4096 -keyout registry.08online.xsg.key -out registry.08online.xsg.crt 12345678910111213Country Name (2 letter code) [AU]:State or Province Name (full name) [Some-State]:Locality Name (eg, city) []:Organization Name (eg, company) [Internet Widgits Pty Ltd]:Organizational Unit Name (eg, section) []:Common Name (e.g. server FQDN or YOUR name) []:IPEmail Address []: # Change Directory and Modify harbor.cfg123ubuntu@ubuntu-xenial:~$ cd harborubuntu@ubuntu-xenial:~/harbor$ vim harbor.cfg 123456789101112131415161718192021&lt; hostname = reg.mydomain.com&gt; hostname = registry.08online.xsg &lt; ui_url_protocol = http&gt; ui_url_protocol = https &lt; ssl_cert = /data/cert/server.crt&gt; ssl_cert = /home/ubuntu/cert/registry.08online.xsg.crt &lt; ssl_cert_key = /data/cert/server.key&gt; ssl_cert_key = /home/ubuntu/cert/registry.08online.xsg.key # Harbor has been installed1ubuntu@ubuntu-xenial:~/harbor$ sudo ./install.sh # Check Containers for Harbor1ubuntu@ubuntu-xenial:~/harbor$ sudo docker-compose top #WebUI(https://IP) 帳號：admin 密碼：xxxxx 安裝 Certificate# 修改憑證，需用公司憑證取代 123ubuntu@ubuntu-xenial:~/harbor$ vim /home/ubuntu/cert/registry.08online.xsg.crt ubuntu@ubuntu-xenial:~/harbor$ vim /home/ubuntu/cert/registry.08online.xsg.key # 修改Docker login需要之憑證 1234567ubuntu@ubuntu-xenial:~/harbor$ mkdir -p /etc/docker/certs.d/registry.08online.xsg/ ubuntu@ubuntu-xenial:~/harbor$ vim /etc/docker/certs.d/registry.08online.xsg/ca.crt ubuntu@ubuntu-xenial:~/harbor$ vim /etc/docker/certs.d/registry.08online.xsg/client.certubuntu@ubuntu-xenial:~/harbor$ vim /etc/docker/certs.d/registry.08online.xsg/client.key # 測試登入1ubuntu@ubuntu-xenial:~/harbor$ docker login registry.08online.xsg 帳號：admin 密碼：xxxxxx LDAP設定# 修改設定檔 1234567891011121314151617181920212223242526272829303132333435363738ubuntu@ubuntu-xenial:~/harbor$ vim harbor.cfg &lt; auth_mode = db_auth&gt; auth_mode = ldap_auth &lt; ldap_url = ldaps://ldap.mydomain.com&gt; ldap_url = ldap://xxx.xxx.xxx:xxx &lt; #ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com&gt; ldap_searchdn = CN=xxxxxxxx,OU=PublicID,OU=Account,DC=xxx,DC=xxx &lt; #ldap_search_pwd = password&gt; ldap_search_pwd = xxxxxxxx &lt; ldap_basedn = ou=people,dc=mydomain,dc=com&gt; ldap_basedn = dc=xxx,dc=xxx &lt; ldap_uid = uid&gt; ldap_uid = sAMAccountName # 重啟服務並強制清除data目錄下資料 1234567ubuntu@ubuntu-xenial:~/harbor$ docker-compose down -vubuntu@ubuntu-xenial:~/harbor$ rm -rf /dataubuntu@ubuntu-xenial:~/harbor$ ./prepareubuntu@ubuntu-xenial:~/harbor$ docker-compose up -d # 先使用管理者帳號登入 1ubuntu@ubuntu-xenial:~/harbor$ docker login registry.08online.xsg 帳號：admin 密碼：xxxxxx # 登入Web-UI調整設定 Configuration &gt; Authentication 測試連線成功後，即可使用AD帳號登入 Push、Pull# 登入 1ubuntu@ubuntu-xenial:~/harbor$ docker login registry.08online.xsg # 幫需上傳的image加上tag 1ubuntu@ubuntu-xenial:~/harbor$ docker tag redis:latest registry.08online.xsg/redis:latest # 上傳 1ubuntu@ubuntu-xenial:~/harbor$ docker push registry.08online.xsg/library/redis:latest # 下載 1ubuntu@ubuntu-xenial:~/harbor$ docker pull registry.08online.xsg/library/redis:latest","categories":[],"tags":[{"name":"Harbor","slug":"Harbor","permalink":"http://yoursite.com/tags/Harbor/"}]},{"title":"Prometheus","slug":"Prometheus","date":"2018-06-19T10:26:03.000Z","updated":"2018-07-19T09:12:39.595Z","comments":true,"path":"2018/06/19/Prometheus/","link":"","permalink":"http://yoursite.com/2018/06/19/Prometheus/","excerpt":"Prometheus特點多維數據架構 (由 metric 名稱與 key/ value 定義的時間序列) 靈活的查詢語言(PromQL) 支援Local與Remote，不依賴分散式儲存 使用Pull方式取得目標資訊，通過HTTP協議傳輸 支援Pushgateway 支援多種圖形模式與Dashboard","text":"Prometheus特點多維數據架構 (由 metric 名稱與 key/ value 定義的時間序列) 靈活的查詢語言(PromQL) 支援Local與Remote，不依賴分散式儲存 使用Pull方式取得目標資訊，通過HTTP協議傳輸 支援Pushgateway 支援多種圖形模式與Dashboard Prometheus核心元件Prometheus Server：主要服務，用來抓取和儲存時序資料 (Time Series Data) Client librarys：用於對接Prometheus Server，負責檢測應用程序代碼，讓其可以透過HTTP傳送資訊到Prometheus Server PushGateway：支援Client主動將短期和批量的監控數據推送至此，而Prometheus會定時來抓取數據 Exporter：負責從目標收集數據，並轉換成Prometheus支援的格式，不同的Exporter負責不同的業務， 命名格式為xxx_exporter Alertmanager：用於告警通知管理 Prometheus架構 Prometheus VS InfluxDBInfluxDB：僅僅是一個資料庫，它被動的接受客戶端資料和查詢請求，基於 Push Prometheus：完整的監控系統，能抓取資料、查詢資料、告警等功能，基於 Pull Push 和 Pull 主要區別在發起者不同及邏輯架構不同 PromQL在Node Exporter的/metrics接口中返回的監控數據，在Prometheus下稱為一個樣本。採集到的樣本由以下三部分組成： 指標（metric） 時間戳記（timestamp） 樣本值（value） Metric TypeCounter（計數器） rate(http_requests_total[5m]) Gauge（儀表板） node_memory_MemFree predict_linear(node_filesystem_free{job=”kubernetes-node-exporter”}[1h], 4 * 3600) Histogram（直方圖） Summary（摘要） 環境準備Ubuntu 16.04 LTS Docker 1.10+ Kubernetes v1.9.6 Node_exporter v0.15+ Prometheus v2.0.0 Grafana 5.1+ 安裝PrometheusCreate A Namespace： 1kubectl create namespace kube-ops Create Exporter： 1kubectl create -f node-exporter.yaml node-exporter.yaml 123456789101112131415161718192021222324252627282930313233343536373839---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: node-exporter namespace: kube-ops labels: k8s-app: node-exporterspec: template: metadata: labels: k8s-app: node-exporter spec: containers: - image: prom/node-exporter name: node-exporter ports: - containerPort: 9100 protocol: TCP name: http---apiVersion: v1kind: Servicemetadata: labels: k8s-app: node-exporter name: node-exporter namespace: kube-opsspec: ports: - name: http port: 9100 nodePort: 31672 protocol: TCP type: NodePort selector: k8s-app: node-exporter Create ServiceAccount、ClusterRole、ClusterRoleBinding： 1kubectl create -f prometheus-sa.yaml prometheus-sa.yaml 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: kube-ops---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: prometheus namespace: kube-opsrules:- apiGroups: [\"\"] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"]- nonResourceURLs: [\"/metrics\"] verbs: [\"get\"]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: prometheus namespace: kube-opsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: kube-ops Create A Config Map： 1kubectl create -f prometheus-cm.yaml prometheus-cm.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-opsdata: prometheus.yml: | global: scrape_interval: 30s scrape_timeout: 30s rule_files: - /etc/prometheus/rules.yml alerting: alertmanagers: - static_configs: - targets: [\"localhost:9093\"] scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: 'kubernetes-cadvisor' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: 'kubernetes-node-exporter' scheme: http tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - source_labels: [__meta_kubernetes_role] action: replace target_label: kubernetes_role - source_labels: [__address__] regex: '(.*):10250' replacement: '$&#123;1&#125;:31672' target_label: __address__ - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name rules.yml: | groups: - name: test-rule rules: - alert: NodeFilesystemUsage expr: (node_filesystem_size&#123;device=\"rootfs\"&#125; - node_filesystem_free&#123;device=\"rootfs\"&#125;) / node_filesystem_size&#123;device=\"rootfs\"&#125; * 100 &gt; 80 for: 2m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: High Filesystem usage detected\" description: \"&#123;&#123;$labels.instance&#125;&#125;: Filesystem usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;\" - alert: NodeMemoryUsage expr: (node_memory_MemTotal - (node_memory_MemFree+node_memory_Buffers+node_memory_Cached )) / node_memory_MemTotal * 100 &gt; 80 for: 2m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: High Memory usage detected\" description: \"&#123;&#123;$labels.instance&#125;&#125;: Memory usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;\" - alert: NodeCPUUsage expr: (100 - (avg by (instance) (irate(node_cpu&#123;job=\"kubernetes-node-exporter\",mode=\"idle\"&#125;[5m])) * 100)) &gt; 80 for: 2m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: High CPU usage detected\" description: \"&#123;&#123;$labels.instance&#125;&#125;: CPU usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;\" - alert: test expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total&#123;job=\"kubernetes-node-exporter\",mode=\"idle\"&#125;[5m])) * 100)) &gt; 1 for: 2m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: High CPU usage detected\" description: \"&#123;&#123;$labels.instance&#125;&#125;: CPU usage is above 1% (current value is: &#123;&#123; $value &#125;&#125;\"---kind: ConfigMapapiVersion: v1metadata: name: alertmanager namespace: kube-opsdata: config.yml: |- global: resolve_timeout: 5m route: receiver: webhook group_wait: 30s group_interval: 5m repeat_interval: 4h group_by: [alertname] routes: - receiver: webhook group_wait: 10s match: team: node receivers: - name: webhook webhook_configs: - url: 'http://apollo/hooks/dingtalk/' send_resolved: true - url: 'http://apollo/hooks/prome/' send_resolved: true Create A Prometheus Deployment： 1kubectl create -f prometheus-deploy.yaml prometheus-deploy.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: k8s-app: prometheus name: prometheus namespace: kube-opsspec: replicas: 1 template: metadata: labels: k8s-app: prometheus spec: serviceAccountName: prometheus containers: - image: prom/prometheus:v2.0.0-rc.3 name: prometheus command: - \"/bin/prometheus\" args: - \"--config.file=/etc/prometheus/prometheus.yml\" - \"--storage.tsdb.path=/prometheus\" - \"--storage.tsdb.retention=24h\" ports: - containerPort: 9090 protocol: TCP name: http volumeMounts: - mountPath: \"/prometheus\" name: data - mountPath: \"/etc/prometheus\" name: config-volume resources: requests: cpu: 100m memory: 100Mi limits: cpu: 200m memory: 1Gi - image: quay.io/prometheus/alertmanager:v0.12.0 name: alertmanager args: - \"-config.file=/etc/alertmanager/config.yml\" - \"-storage.path=/alertmanager\" ports: - containerPort: 9093 protocol: TCP name: http volumeMounts: - name: alertmanager-config-volume mountPath: /etc/alertmanager resources: requests: cpu: 50m memory: 50Mi limits: cpu: 200m memory: 200Mi volumes: - name: data emptyDir: &#123;&#125; - configMap: name: prometheus-config name: config-volume - name: alertmanager-config-volume configMap: name: alertmanager Exposing Prometheus As A Service： 1kubectl create -f prometheus-svc.yaml prometheus-svc.yaml 123456789101112131415apiVersion: v1kind: Servicemetadata: name: prometheus namespace: kube-ops labels: k8s-app: prometheusspec: selector: k8s-app: prometheus type: NodePort ports: - name: web port: 9090 targetPort: http Create A Grafana Deployment： 1kubectl create -f grafana-deployment.yaml grafana-deployment.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: grafana namespace: kube-ops labels: app: grafana #component: corespec: replicas: 1 template: metadata: labels: app: grafana #component: core spec: containers: - image: grafana/grafana:latest securityContext: runAsUser: 0 name: grafana imagePullPolicy: IfNotPresent # env: resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi env: # The following env variables set up basic auth twith the default admin user and admin password. - name: GF_AUTH_BASIC_ENABLED value: \"true\" - name: GF_AUTH_ANONYMOUS_ENABLED value: \"false\" # - name: GF_AUTH_ANONYMOUS_ORG_ROLE # value: Admin # does not really work, because of template variables in exported dashboards: # - name: GF_DASHBOARDS_JSON_ENABLED # value: \"true\" readinessProbe: httpGet: path: /login port: 3000 # initialDelaySeconds: 30 # timeoutSeconds: 1 volumeMounts: - name: grafana-persistent-storage mountPath: /var/lib/grafana volumes: - name: grafana-persistent-storage emptyDir: &#123;&#125; Exposing the Grafana web UI： 1kubectl create -f grafana-svc.yaml grafana-svc.yaml 123456789101112131415apiVersion: v1kind: Servicemetadata: name: grafana namespace: kube-ops labels: app: grafana #component: corespec: type: NodePort ports: - port: 3000 selector: app: grafana #component: core Prometheus UI介面Grafana設定","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://yoursite.com/categories/DevOps/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"http://yoursite.com/tags/Prometheus/"}]},{"title":"Prometheus (Docker)","slug":"Prometheus-Docker","date":"2018-03-26T07:25:46.000Z","updated":"2018-07-19T09:23:48.631Z","comments":true,"path":"2018/03/26/Prometheus-Docker/","link":"","permalink":"http://yoursite.com/2018/03/26/Prometheus-Docker/","excerpt":"環境準備Ubuntu 16.04 LTS Docker 1.12+","text":"環境準備Ubuntu 16.04 LTS Docker 1.12+ 前置安裝# Check Version1sudo apt-get install -y docker.io # Check Docker Version1sudo docker version 安裝 mysqld_exporter# 啟動mysqld_exporter容器123docker run --name mysqld_exporter -d -p 9104:9104 --restart=always \\ -e DATA_SOURCE_NAME=&quot;exporter:exporter@(localhost:3306)/mysql&quot; \\ prom/mysqld-exporter # 查看容器日誌，確定容器正常運行1docker logs -f mysqld_exporter # 確認exporter有導出數據 瀏覽器訪問http://localhost:9104 安裝prometheus# 創建資料夾1mkdir -p /home/docker/prometheus # 創建Data資料夾1mkdir -p /home/docker/prometheus/prometheus-data # 編輯yml文件1vim /home/docker/prometheus/prometheus.yml # 配置文檔如下1234567891011121314151617181920212223242526272829303132333435# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] labels: instance: prometheus - job_name: 'mysql' scrape_interval: 15s static_configs: - targets: - 'localhost:9104' labels: instance: db1 # 啟動prometheus容器1234docker run --name prometheus -d -p 9090:9090 --restart=always \\ -v /home/docker/prometheus/prometheus-data:/prometheus-data \\ -v /home/docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus # 確認Targets 瀏覽器訪問http://localhost:9090/targets 安裝Grafana# 創建資料夾1mkdir -p /home/docker/grafana # 創建Data資料夾1mkdir -p /home/docker/grafana/data # 啟動grafana容器1234docker run --name grafana -d -p 3000:3000 --restart=always \\ -e &quot;GF_SECURITY_ADMIN_PASSWORD=admin&quot; \\ -v /home/docker/grafana/data:/var/lib/grafana \\ grafana/grafana # 登入grafana介面 瀏覽器訪問http://localhost:3000 帳號admin 密碼admin # Add deta source # 推薦Dashboards 3622 2 358 159","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://yoursite.com/categories/DevOps/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"http://yoursite.com/tags/Prometheus/"}]}]}